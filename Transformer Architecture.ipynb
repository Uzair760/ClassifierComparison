{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e729f625",
   "metadata": {},
   "source": [
    "# Transformer Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d84c3",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Positional encoding provides positional information for each token in a sequence. These are required by the attention mechanisms. A positional encoding vector is calculated using the token embedding, and is made unique with the sine/cosine functions. The position information is the sum of the positional encoding vector and the token embedding.\n",
    "\n",
    "### Attention Mechanism\n",
    "#### Self attention\n",
    "Self-attention provides the model with information on the relationship between words in a sequence. Query, Key and Value matrices are linear projections of the token embeddings, each trained with their own weights. The dot product of Query and Key matrices provides the attention scores between words. The softmax function then produces weights to provide some tokens with higher attention. These weights are applied on the Value matrix to update the token embeddings. \n",
    "#### Multi-Head Attention\n",
    "The above self attention is applied multiple times, in parallel, to learn different semantic aspects of the sequence. The outputs from each head are concatenated and a linear layer provides the updated token embedding.\n",
    "#### Feed Forward Layer\n",
    "The feed forward layers map the knowledge gained in the attention stage, into non-linear representations of our data, through use of the ReLU function. A separate embedding dimension (d_ff) is used to help capture more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding using the nn.Module subclass\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length        \n",
    "        # Create a positional encoding matrix up to the specified max_length\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        # Scale position indices and calculate position encodings into the matrix\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # Set matrix as a non-trainable parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    # Add the positional encodings to the embeddings tensor\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Multi-Head Attention Mechanism\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Set the number of attention heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\t\t# Set up the linear transformations for Q, K, V\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        # Set the layer for the final concatenated output\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Split the sequence embeddings in x across the attention heads to ensure the outputs have the correct dimensions\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
    "\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # Compute Q-K pair dot-product attention scores\n",
    "        scores = torch.matmul(query, key.permute(1, 2, 0))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # Use softmax to normalize attention scores into attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "        # Calculate the attention weights\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "        # Multiply attention weights by values and concatenate outputs\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        # Linearly project the outputs\n",
    "        return self.output_linear(output)\n",
    "    \n",
    "# Feed Forward layer\n",
    "class FeedForwardSubLayer(nn.Module):\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        # Apply a ReLU layer for non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103762ce",
   "metadata": {},
   "source": [
    "# Transformer Encoder Architecture\n",
    "#### Encoder Layer\n",
    "The encoder layer applies the Multi-Head Attention mechanism and the Feed Forward (Sub)layer. Normalization layers are used to keep the outputs on a similar scale. Dropout layers are used to regulate the training process by reducing overfitting of our model on the training data. A mask is used to prevent processing of padding tokens when dealing with different length sequences.\n",
    "#### Encoder Transformer Head\n",
    "The transformer head processes the encoded inputs and produces a task specific output. In our example, we have a:\n",
    "- Transformer classification head for extractive question-answering, text classification, sentiment analysis etc.\n",
    "- Transformer regression head for language complexity, estimating text readability etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder layer class\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Apply the forward pass\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        # Perform for all encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "    \n",
    "# Transformer classification head\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "# Transformer regression head\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, d_model, output_dim):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369af7c",
   "metadata": {},
   "source": [
    "## Training and Testing the Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.randint(1, train_vocab_size, (5, max_len))\n",
    "target_data = torch.randint(1, target_vocab_size, (5, max_len))\n",
    "\n",
    "model = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train our model\n",
    "for epoch in range(epoch_num): \n",
    "    # Zero the gradient before each pass\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data, target_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, target_vocab_size), target_data[:, 1:].contiguous().view(-1))\n",
    "    # Compute loss and update parameters\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss= {loss.item():.3f}\")\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Complete the forward pass \n",
    "output = encoder(input_sequence, mask)\n",
    "classification = classifier(output)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4682eff",
   "metadata": {},
   "source": [
    "# Transformer Decoder Architecture\n",
    "#### Decoder Layer - Masked Multi-Head Self Attention\n",
    "The decoder layer is similar to the encoder layer, but it also uses Masked Multi-Head Self Attention. In this mechanism, an upper triangular mask is used, meaning that for each token, only the previously generated tokens are looked at and future tokens are not considered. This helps in predicting the next word in the sequence, one token at a time.\n",
    "\n",
    "#### Decoder Transformer Head\n",
    "A linear layer and a softmax activation function is applied, over the entire vocabulary, predicting the most likely next token. \n",
    "# Transformer Encoder-Decoder Architecture\n",
    "#### Output Embeddings (Decoder Inputs)\n",
    "In practice, the decoder only takes the target sequences as training data, e.g. output summarisation in summarisation tasks, next words in text generation tasks etc. When using the model, the output embedding is empty and is as the next tokens are generated.\n",
    "#### Cross Attention Mechanism\n",
    "This occurs in the decoder layer, after the masked attention step. Final hidden states from the encoder and information passed through the decoder are taken as inputs. The encoder outputs (y) are provided as the key and value arguments, to allow the decoder to consider the processed input sequence. The decoder (x) provides the cross-attention query, to help generate the target sequence.\n",
    "#### Encoder-Decoder Transformer Head\n",
    "A linear layer and a softmax activation function is applied, providing probabilities for the next-word from the decoder output. Other activations may be used instead of softmax depending on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder layer class\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()     \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads) # Not required in decoder only\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        # Apply the transformer head for next-word prediction inside this class\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22244c42",
   "metadata": {},
   "source": [
    "## Testing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e001361",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a triangular attention mask for causal attention by applying an upper triangular matrix\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
    "\n",
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd7100",
   "metadata": {},
   "source": [
    "## Testing the Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff19544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
